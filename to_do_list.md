# To do list 

    ❥ Day 1

        1) Make a first draft of the code : 
            [ ]  How to get all urls ? -> Done by Melih
            [ ]  How to scrape each of the urls ? -> Done by Paul & Ramina
            [ ]  How to connect urls from first pages to later scrap all the urls ? -> Done by Melih & Ramina

        2) Look for information on multithreading & multiprocessing to have better efficiency : 
            [ ] How to change our code to make it more efficient : Done by Yuliia
            [ ]  How to change our code to make it work with concurrent.futures.ThreadPoolExecutor -> to be continued by Melih

    ❥ Day 2

        1) Change part 1 of the code. Look for information on multithreading & multiprocessing to have better efficiency :
            [ ]  Should we use multithreading or multiprocessing ?  -> To do by Melih
            [ ]  Should we use Selenium or requests/sessions or else ? Which is most efficient ?  -> To do by Melih
            [ ] Export urls file and push to repo. -> Done by Melih

        2) Part 2 of the code. Write script to scrape the fixed header of immoweb : 
            [ ]   Srape the header -> to do Yuliia
            [ ]   How to add the scraped data from the fixed header to the pandas Dataframe -> to do Yuliia

        3) Change the part 2 of the code :
            [ ]   Rewrite the code with requests or sessions (what works better). -> to do by Paul
            [ ]   Change the dataframe structure to have caracteristics as columns and each property as a row.  -> to do by Paul 
            [ ]   Add the code on dissecting the url to make a caracteristic out of it and add to dataframe.  -> to do by Paul

        4) Find a way to multithread the part 2 of the code :
            [ ]   See how the code can be updated to transform it into a function and then use it with map on a sample of urls list. -> to do by Melih


    ❥ Day 3

            [ ]   Group decision : decide how the extracted tables are going to be either extract all info or format it ?
            [ ]   We still need to extract the address from the fixed header or decide to take the city in the url.
            [ ]   Code dissecting the url and add columns with caract & value extracted from url.
            [ ]   See how the code can be updated to transform it into a function and then use it with map on a sample of urls list.
            [ ]   Clean the dataframe (take out empty rows, transform Yes/no into bianry 1/0, take out all the metric measure m² etc, make sure number are int or float and not strings)
            [ ]   Start writing the README file.





                


    

